{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open('Doc1.txt')\n",
    "my_lines_list1=file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('doc2.txt')\n",
    "my_lines_list2=file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"food is ani substanc [ 1 ] consum to provid nutrit support for an organ . food is usual of plant or anim origin , and contain essenti nutrient , such as carbohydr , fat , protein , vitamin , or miner . the substanc is ingest by an organ and assimil by the organ 's cell to provid energi , maintain life , or stimul growth . \",\n",
       " '',\n",
       " 'histor , human secur food through two method : hunt and gather and agricultur , which gave modern human a mainli omnivor diet . worldwid , human ha creat numer cuisin and culinari art , includ a wide array of ingredi , herb , spice , techniqu , and dish . ',\n",
       " '',\n",
       " 'today , the major of the food energi requir by the ever-increas popul of the world is suppli by the food industri . food safeti and food secur are monitor by agenc like the intern associ for food protect , world resourc institut , world food programm , food and agricultur organ , and intern food inform council . they address issu such as sustain , biolog divers , climat chang , nutrit econom , popul growth , water suppli , and access to food . ',\n",
       " '',\n",
       " \"the right to food is a human right deriv from the intern coven on econom , social and cultur right ( icescr ) , recogn the `` right to an adequ standard of live , includ adequ food '' , as well as the `` fundament right to be free from hunger '' . mani plant and plant part are eaten as food and around 2,000 plant speci are cultiv for food . mani of these plant speci have sever distinct cultivar . [ 7 ] \",\n",
       " '',\n",
       " \"seed of plant are a good sourc of food for anim , includ human , becaus they contain the nutrient necessari for the plant 's initi growth , includ mani health fat , such as omega fat . In fact , the major of food consum by human be are seed-bas food . edibl seed includ cereal ( corn , wheat , rice , et cetera ) , legum ( bean , pea , lentil , et cetera ) , and nut . oilse are often press to produc rich oil - sunflow , flaxse , rapese ( includ canola oil ) , sesam , et cetera . [ 8 ] \",\n",
       " '',\n",
       " 'seed are typic high in unsatur fat and , in moder , are consid a health food . howev , not all seed are edibl . larg seed , such as those from a lemon , pose a choke hazard , while seed from cherri and appl contain cyanid which could be poison onli if consum in larg volum . [ 9 ] ',\n",
       " '',\n",
       " 'fruit are the ripen ovari of plant , includ the seed within . mani plant and anim have coevolv such that the fruit of the former are an attract food sourc to the latter , becaus anim that eat the fruit may excret the seed some distanc away . fruit , therefor , make up a signific part of the diet of most cultur . some botan fruit , such as tomato , pumpkin , and eggplant , are eaten as veget . [ 10 ] ( for more inform , see list of fruit . ) ',\n",
       " '',\n",
       " 'veget are a second type of plant matter that is commonli eaten as food . these includ root veget ( potato and carrot ) , bulb ( onion famili ) , leaf veget ( spinach and lettuc ) , stem veget ( bamboo shoot and asparagu ) , and infloresc veget ( globe artichok and broccoli and other veget such as cabbag or cauliflow ) . [ 11 ] ']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "stem_sentence_list1=[]\n",
    "stem_sentence_list2=[]\n",
    "for line in my_lines_list1:\n",
    "    stem_sentence=stemSentence(line)\n",
    "    stem_sentence_list1.append(stem_sentence);\n",
    "for line in my_lines_list2:\n",
    "    stem_sentence=stemSentence(line)\n",
    "    stem_sentence_list2.append(stem_sentence);   \n",
    "stem_sentence_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(binary=True)\n",
    "vect.fit(stem_sentence_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29957234 0.         0.29735052 0.         0.29773258 0.\n",
      "  0.36028835]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.27914526 0.         0.25860327 0.         0.17654697 0.\n",
      "  0.08703883]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.19090089 0.         0.21222324 0.         0.21732538 0.\n",
      "  0.35714286]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.28203804 0.         0.44791401 0.         0.50964719 0.\n",
      "  0.37688918]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.2123977  0.         0.31974733 0.         0.2686642  0.\n",
      "  0.39735971]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.1300665  0.         0.21086633 0.         0.24678382 0.\n",
      "  0.24333213]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.18531233 0.         0.30901572 0.         0.37504578 0.\n",
      "  0.31201886]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.21398025 0.         0.26761547 0.         0.24359938 0.\n",
      "  0.24019223]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(vect.transform(stem_sentence_list1).toarray(), vect.transform(stem_sentence_list2).toarray())\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2\n",
    "file =open('Doc1.txt')\n",
    "my_lines_list1=file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('doc2.txt')\n",
    "my_lines_list2=file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(binary=True)\n",
    "vect.fit(my_lines_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33796318 0.         0.29735052 0.         0.30207927 0.\n",
      "  0.33075929]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.15512631 0.         0.22292541 0.         0.14705882 0.\n",
      "  0.08856149]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.21320072 0.         0.23635158 0.         0.21828206 0.\n",
      "  0.32863353]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.26673253 0.         0.41068926 0.         0.42986348 0.\n",
      "  0.38069349]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.18993429 0.         0.28074496 0.         0.21606791 0.\n",
      "  0.35783003]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.1467348  0.         0.21086633 0.         0.2503867  0.\n",
      "  0.25131234]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.1672484  0.         0.30901572 0.         0.35673862 0.\n",
      "  0.32225169]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.24140227 0.         0.23788041 0.         0.21969401 0.\n",
      "  0.24806947]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(vect.transform(my_lines_list1).toarray(), vect.transform(my_lines_list2).toarray())\n",
    "print(similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
